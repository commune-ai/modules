{
    "code": {
        "agent/agent.py": "import commune as c\nimport os\nimport json\n\nclass Agent:\n    def __init__(self, \n                 model='google/gemini-2.5-pro-exp-03-25:free',\n                 tools = ['put_text',   \n                            'files', \n                            'file2text', \n                            'get_text', \n                            'get_json', \n                            'put_json', \n                            'ls', \n                            'glob'],\n                 max_tokens=420000, \n                 prompt = 'The following is a conversation with an AI assistant. The assistant is helpful, creative, clever, and very friendly.',\n                **kwargs):\n        \n        self.max_tokens = max_tokens\n        self.prompt = prompt\n        self.model = c.module('model.openrouter')(model=model, **kwargs)\n\n    def forward(self, text = 'whats 2+2?' ,  \n                    temperature= 0.5,\n                    max_tokens= 1000000, \n                    preprocess=True,\n                    stream=True,\n                    **kwargs):\n        if preprocess:\n            text = self.preprocess(text)\n        params = {'message': text, 'temperature': temperature, 'max_tokens': max_tokens,  'stream': stream, **kwargs}\n        return self.model.forward(**params)\n\n    def ask(self, *text, **kwargs): \n        return self.forward(' '.join(list(map(str, text))), **kwargs)\n\n    def models(self, *args, **kwargs):\n        return self.model.models(*args,**kwargs)\n\n    def resolve_path(self, path):\n        return os.path.expanduser('~/.commune/agent/' + path)\n\n\n\n    def preprocess(self, text, threshold=1000):\n            new_text = ''\n            is_function_running = False\n            words = text.split(' ')\n            fn_detected = False\n            fns = []\n            for i, word in enumerate(words):\n                prev_word = words[i-1] if i > 0 else ''\n                # restrictions can currently only handle one function argument, future support for multiple\n                magic_prefix = f'@/'\n                if word.startswith(magic_prefix) and not fn_detected:\n                    word = word[len(magic_prefix):]\n                    if '/' not in word:\n                        word = '/' + word\n                    fns += [{'fn': word, 'params': [], 'idx': i + 2}]\n                    fn_detected=True\n                else:\n                    if fn_detected:\n                        fns[-1]['params'] += [word]\n                        fn_detected = False\n            c.print(fns)\n            for fn in fns:\n                print('Running function:', fn)\n                result = c.fn(fn['fn'])(*fn['params'])\n                fn['result'] = result\n                text =' '.join([*words[:fn['idx']],'-->', str(result), *words[fn['idx']:]])\n            return text\n        ",
        "agent/app.py": "import commune as c\nimport streamlit as st\n\n\nclass App(c.Module):\n\n    def __init__(self, \n                 max_tokens=420000, \n                 password = None,\n                 text = 'Hello whaduop fam',\n                 system_prompt = 'The following is a conversation with an AI assistant. The assistant is helpful, creative, clever, and very friendly.',\n                 model = None,\n                 history_path='history',\n                **kwargs):\n\n        self.max_tokens = max_tokens\n        self.text = text\n        self.set_module(model, \n                        password = password,\n                        history_path=history_path, \n                        system_prompt=system_prompt,\n                        **kwargs)\n        \n    def set_module(self,\n                    model, \n                   history_path='history', \n                   password=None,\n                   system_prompt = 'The following is a conversation with an AI assistant. The assistant is helpful, creative, clever, and very friendly.',\n                    **kwargs):\n        self.system_prompt = system_prompt\n        self.admin_key = c.pwd2key(password) if password else self.key\n        self.model = c.module('agent')(model=model)\n        self.models = self.model.models()\n        self.history_path = self.resolve_path(history_path)\n        return {'success':True, 'msg':'set_module passed'}\n    \n    def add_files(self, files):\n        cwd = st.text_input('cwd', './')\n        files = c.glob(cwd)\n        files = st.multi_select(files, 'files')\n        file_options  = [f.name for f in files]\n\n\n    def call(self, \n            input = 'whats 2+2?' ,\n            temperature= 0.5,\n            max_tokens= 1000000,\n            model= 'anthropic/claude-3.5-sonnet', \n            system_prompt= 'make this shit work',\n            key = None,\n            stream=True, \n            ):\n        # key = self.resolve_key(key)\n        data = c.locals2kwargs(locals())\n        signature = self.key.ticket(c.hash(data))\n        return signature\n    \n    def sidebar(self, user='user', password='password', seperator='::'):\n        with st.sidebar:\n            st.title('Just Chat')\n            # assert self.key.verify_ticket(ticket)\n            with st.expander('LOGIN'): \n                cols = st.columns([1,1])\n                user_name = cols[0].text_input('User', user)\n                pwd = cols[1].text_input('Password', password, type='password')\n                seed = c.hash(user_name + seperator + pwd)\n                self.key = c.pwd2key(seed)\n                self.data = c.dict2munch({  \n                                'user': user_name, \n                                'path': self.resolve_path('history', self.key.ss58_address ),\n                                'history': self.history(self.key.ss58_address)\n                                })\n    \n    def search_history(self):\n        search = st.text_input('Search')\n        # if the search is in any of the columns\n        history = c.copy(self.data.history)\n\n        history = [h for h in history if search in str(h)]\n        df = c.df(history)\n        st.write(df)\n\n    @classmethod \n    def run(cls):\n        self = cls()\n        self.sidebar()\n        tab_names = ['Chat', 'History']\n        tabs = st.tabs(tab_names)\n        with tabs[0]:\n            self.chat_page()\n        with tabs[1]:\n            self.history_page()\n\n    def chat_page(self):\n        with st.sidebar.expander('PARAMS', expanded=True):\n            model = st.selectbox('Model', self.models)\n            temperature = st.number_input('Temperature', 0.0, 1.0, 0.5)\n            if hasattr(self.model, 'get_model_info'):\n                model_info = self.model.get_model_info(model)\n                max_tokens = min(int(model_info['context_length']*0.9), self.max_tokens)\n            else:\n                model_info = {}\n                max_tokens = self.max_tokens\n            max_tokens = st.number_input('Max Tokens', 1, max_tokens, max_tokens)\n            system_prompt = st.text_area('System Prompt',self.system_prompt, height=200)\n\n        input  = st.text_area('Text',self.text, height=100)\n        input = input + '\\n' + system_prompt\n\n\n        params = {\n            'input': input,\n            'model': model,\n            'temperature': temperature,\n            'max_tokens': max_tokens,\n        }\n\n        cols = st.columns([1,1])\n        send_button = cols[0].button('Send', key='send', use_container_width=True) \n        stop_button = cols[1].button('Stop', key='stop', use_container_width=True)\n        if send_button and not stop_button:\n            r = self.model.generate(params['input'], \n                                    max_tokens=params['max_tokens'], \n                                    temperature=params['temperature'], \n                                    model=params['model'],\n                                    stream=True)\n            # dank emojis to give it that extra flair\n            emojis = '\u2705\ud83e\udd16\ud83d\udcbb\ud83d\udd0d\ud83e\udde0\ud83d\udd27\u2328\ufe0f'\n            reverse_emojis = emojis[::-1]\n            with st.spinner(f'{emojis} Generating {reverse_emojis}'):\n                st.write_stream(r)\n\n    def post_processing(self, data):\n        lambda_string = st.text_area('fn(x={model_output})', 'x', height=100)\n        prefix = 'lambda x: '\n        lambda_string = prefix + lambda_string if not lambda_string.startswith(prefix) else lambda_string\n        lambda_fn = eval(lambda_string)\n        try:\n            output = data['data']['output']\n            output = lambda_fn(output)\n        except Exception as e:\n            st.error(e)\n\n    def history_page(self):\n        history = self.data.history\n        if len(history) == 0:\n            st.error('No History')\n            return\n        else:\n            cols = history[0].keys()\n            selected_columns = st.multiselect('Columns', cols, cols)\n            df = c.df(history)[selected_columns]\n            st.write(df)\n    def user_files(self):\n        return c.get(self.data['path'])\n\n\n\n    def save_data(self, address, data):\n        return c.put(self.history_path + '/' + address +'/data.json', data)\n    \n    def get_data(self, address):\n        return c.get(self.history_path + '/' + address +'/data.json')\n\n        \n    def clear_history(self, address):\n        return c.rm(self.history_path +  '/'+ address)\n    \n    def history_paths(self, address:str=None):\n        paths = []\n        if address == None:\n            for user_address in self.user_addresses():\n                 paths += self.history_paths(user_address)\n        else:\n            paths = c.ls(self.history_path + '/'+ address)\n        return paths\n    \n    def save_data(self, data):\n        path = self.history_path + '/'+ data['address'] + '/' + str(data['time']) + '.json'\n        return c.put(path, data)\n    \n    def history(self, address:str=None, columns=['datetime', \n                                                 'input', \n                                                 'output', \n                                                 'system_prompt',\n                                                 'model', \n                                                 'temperature',\n                                                   'max_tokens'], df=False):\n        paths = self.history_paths(address)\n        history =  []\n        for i, path in enumerate(paths):\n            try:\n                print(paths)\n                h = c.get(path)\n                h.update(h.pop('data'))\n                h['datetime'] = c.time2datetime(h.pop('time'))\n                h = {k:v for k,v in h.items() if k in columns}\n                history.append(h)\n            except Exception as e:\n                print(e)\n        # sort by time\n    \n        history = sorted(history, key=lambda x: x['datetime'], reverse=True)\n        if df:\n            history = c.df(history)\n        return history\n    \n    def user_addresses(self, display_name=False):\n        users = [u.split('/')[-1] for u in c.ls(self.history_path)]\n        return users\n\nif __name__ == '__main__':\n    App().run()",
        "agent/desc.py": "    \nclass Desc:\n\n    def __init__(self, agent='agent'):\n        self.agent = c.module(agent)()\n        \n    def forward(self, module, max_age=0):\n        code= c.code(module)\n        code_hash = c.hash(code)\n        path = self.resolve_path(f'summary/{module}.json')\n        output = c.get(path, max_age=max_age)\n        if output != None:\n            return output\n\n        prompt = {\n                \"task\": \"summarize the following into tupples and make sure you compress as much as oyu can\",\n                \"code\": code,\n                \"hash\": code_hash,\n                }\n        output = ''\n        for ch in self.agent.forward(str(prompt), preprocess=False):\n            output += ch\n            print(ch, end='')\n        c.put(path, output)\n        print('Writing to path -->', path)\n        return output\n",
        "agent/plan.py": "\n\nimport os\nimport json\nimport commune as c\n\nclass Agent:\n    def __init__(self, agent='agent'):\n        self.agent = c.module(agent)()\n    def forward(self, text, *extra_text, path='./', run=False, **kwargs):\n        text = text + ' '.join(list(map(str, extra_text)))\n        anchors= ['<START_OUTPUT>','<END_OUTPUT>']\n        prompt = f\"\"\"\n\n        INSTRUCTION:\n        YOU NEED TO PLAN THE NEXT SET OF ACTIONS IN THE COMMAND LINE\n        IF YOU ARE UNSURE YOU CAN READ THE FILE AND THEN DECIDE YOU CAN \n        DECIDE TO RUN THE COMMANDS AND WE CAN FEED IT TO YOU AGAIN SO YOU \n        HAVE MULTIPLE CHANCES TO GET IT RIGHT\n        TASK:\n        {text}\n        CONTEXT:\n        {c.files(path)}\n        OUTPUT FORMAT:\n        {anchors[0]}LIST[dict(cmd:str, reason:str)]{anchors[1]}\n        \"\"\"\n\n        output = ''\n        for ch in self.agent.forward(prompt, **kwargs):\n            output += ch\n            print(ch, end='')\n            if anchors[1] in output:\n                break\n        \n        plan =  json.loads(output.split(anchors[0])[1].split(anchors[1])[0])\n\n        if run:\n            input_response = input(f'Run plan? (y/n): {plan}')\n            if input_response.lower() in ['y', 'yes']:\n                for p in plan:\n                    print('Running command:', p['cmd'])\n                    c.cmd(p['cmd'])\n\n        return plan\n\n   ",
        "agent/preprocess.py": "\n\nimport os\nimport json\nimport commune as c\n\nclass Preprocess:\n    def __init__(self, agent='agent'):\n        self.agent = c.module(agent)()\n   ",
        "agent/reduce.py": " \n    \n\n\nimport os\nimport json\nimport commune as c\n\n\nclass Reduce:\n    def __init__(self, agent='agent'):\n        self.agent = c.module(agent)()\n    def forward(self, text, max_chars=10000 , timeout=40, max_age=30, model='openai/o1-mini'):\n        if os.path.exists(text): \n            path = text\n            if os.path.isdir(path):\n                print('REDUCING A DIRECTORY -->', path)\n                future2path = {}\n                path2result = {}\n                paths = c.files(path)\n                progress = c.tqdm(len(paths), desc='Reducing', leave=False)\n                while len(paths) > 0:\n                    for p in paths:\n                        future = c.submit(self.reduce, [p], timeout=timeout)\n                        future2path[future] = p\n                    try:\n                        for future in c.as_completed(future2path, timeout=timeout):\n                            p = future2path[future]\n                            r = future.result()\n                            paths.remove(p)\n                            path2result[p] = r\n                            print('REDUCING A FILE -->', r)\n                            progress.update(1)\n                    except Exception as e:\n                        print(e)\n                return path2result\n            else:\n                assert os.path.exists(path), f'Path {path} does not exist'\n                print('REDUCING A FILE -->', path)\n                text = str(c.get_text(path))\n        elif c.module_exists(text):\n            text = c.code(text)\n\n        original_length = len(text)\n        code_hash = c.hash(text)\n        path = f'summary/{code_hash}' \n\n        text = f'''\n        GOAL\n        summarize the following into tupples and make sure you compress as much as oyu can\n        CONTEXT\n        {text}\n        OUTPUT FORMAT ONLY BETWEEN THE TAGS SO WE CAN PARSE\n        <OUTPUT>DICT(data=List[Dict[str, str]])</OUTPUT>\n        '''\n        if len(text) >= max_chars * 2 :\n            batch_text = [text[i:i+max_chars] for i in range(0, len(text), max_chars)]\n            futures =  [c.submit(self.reduce, [batch], timeout=timeout) for batch in batch_text]\n            output = ''\n            try:\n                for future in c.as_completed(futures, timeout=timeout):\n                    output += str(future.result())\n            except Exception as e:\n                print(e)\n            final_length = len(text)\n            result = { 'compress_ratio': final_length/original_length, \n                      'final_length': final_length, \n                      'original_length': original_length, \n                      \"data\": text}\n            return result\n        if \"'''\" in text:\n            text = text.replace(\"'''\", '\"\"\"')\n        \n        data =  c.ask(text, model=model, stream=0)\n        def process_data(data):\n            try:\n                data = data.split('<OUTPUT>')[1].split('</OUTPUT>')[0]\n                return data\n            except:\n                return data\n        return {\"data\": process_data(data)}\n",
        "agent/score.py": "\n\nimport os\nimport json\nimport commune as c\n\n\n\nclass Score:\n    def __init__(self, agent='agent'):\n        self.agent = c.module(agent)()\n    def forward(self, module:str, **kwargs):\n        if c.path_exists(module):\n            code = c.file2text(module)\n        else:\n            code = c.code(module)\n        \n        prompt = f\"\"\"\n        GOAL:\n        score the code out of 100 and provide feedback for improvements \n        and suggest point additions if they are included to\n        be very strict and suggest points per improvement that \n        you suggest in the feedback\n        YOUR SCORING SHOULD CONSIDER THE FOLLOWING VIBES:\n        - readability\n        - efficiency\n        - style\n        - correctness\n        - comments should be lightlu considered only when it makes snes, we want to avoid over commenting\n        - code should be self explanatory\n        - code should be well structured\n        - code should be well documented\n        CODE: \n        {code}\n        OUTPUT_FORMAT:\n        <OUTPUT>DICT(score:int, feedback:str, suggestions=List[dict(improvement:str, delta:int)]])</OUTPUT>\n        \"\"\"\n        output = ''\n        for ch in  self.agent.forward(prompt, stream=True, **kwargs):\n            output += ch\n            print(ch, end='')\n            if '</OUTPUT>' in output:\n                break\n        return json.loads(output.split('<OUTPUT>')[1].split('</OUTPUT>')[0])\n",
        "agent/verifyfn.py": "\n\n\nimport os\nimport json\nimport commune as c\n\nclass VerifyFn:\n    def forward(self, fn='module/ls'):\n            code = c.code(fn)\n            prompt = {\n                'goal': 'make the params from the following  and respond in JSON format as kwargs to the function and make sure the params are legit',\n                'code': code, \n                'output_format': '<JSON_START>DICT(params:dict)</JSON_END>',\n            }\n            response =  c.ask(prompt, preprocess=False)\n            output = ''\n            for ch in response:\n                print(ch, end='')\n                output += ch\n                if '</JSON_END>' in output:\n                    break\n            params_str = output.split('<JSON_START>')[1].split('</JSON_END>')[0].strip()\n            params = json.loads(params_str)['params']\n\n            result =  c.fn(fn)(**params)\n\n            ## SCORE THE RESULT\n            prompt = {\n                'goal': '''score the code given the result, params \n                and code out of 100 if the result is indeed the result of the code\n                make sure to only respond in the output format''',\n                'code': code, \n                'result': result,\n                'params': params,\n                'output_format': '<JSON_START>DICT(score:int, feedback:str, suggestions=List[dict(improvement:str, delta:int)]</JSON_END>',\n            }\n\n            response =  c.ask(prompt, preprocess=False)\n\n            output = ''\n            for ch in response:\n                print(ch, end='')\n                output += ch\n                if '</JSON_END>' in output:\n                    break\n                \n            return json.loads(output.split('<JSON_START>')[1].split('</JSON_END>')[0].strip())\n"
    },
    "schema": {
        "ask": {
            "input": {
                "self": {
                    "value": "_empty",
                    "type": "_empty"
                },
                "text": {
                    "value": "_empty",
                    "type": "_empty"
                },
                "kwargs": {
                    "value": "_empty",
                    "type": "_empty"
                }
            },
            "output": {
                "value": null,
                "type": "None"
            },
            "docs": null,
            "cost": 1,
            "name": "ask",
            "source": {
                "start": 35,
                "length": 2,
                "path": "~/commune/commune/modules/agent/agent/agent.py",
                "code": null,
                "hash": "sha256:28b089993b2d9998bab4d6ef52760508a5eba61412c8d252b37cbf5cde6b8a4a",
                "end": 37
            }
        },
        "forward": {
            "input": {
                "self": {
                    "value": "_empty",
                    "type": "_empty"
                },
                "text": {
                    "value": "whats 2+2?",
                    "type": "str"
                },
                "temperature": {
                    "value": 0.5,
                    "type": "float"
                },
                "max_tokens": {
                    "value": 1000000,
                    "type": "int"
                },
                "preprocess": {
                    "value": true,
                    "type": "bool"
                },
                "stream": {
                    "value": true,
                    "type": "bool"
                },
                "kwargs": {
                    "value": "_empty",
                    "type": "_empty"
                }
            },
            "output": {
                "value": null,
                "type": "None"
            },
            "docs": null,
            "cost": 1,
            "name": "forward",
            "source": {
                "start": 24,
                "length": 10,
                "path": "~/commune/commune/modules/agent/agent/agent.py",
                "code": null,
                "hash": "sha256:64fcfb35707fe998748952a578877f65024251105a7fafda7a66f70b658ca3c1",
                "end": 34
            }
        },
        "models": {
            "input": {
                "self": {
                    "value": "_empty",
                    "type": "_empty"
                },
                "args": {
                    "value": "_empty",
                    "type": "_empty"
                },
                "kwargs": {
                    "value": "_empty",
                    "type": "_empty"
                }
            },
            "output": {
                "value": null,
                "type": "None"
            },
            "docs": null,
            "cost": 1,
            "name": "models",
            "source": {
                "start": 38,
                "length": 2,
                "path": "~/commune/commune/modules/agent/agent/agent.py",
                "code": null,
                "hash": "sha256:9b258114b27d875bfc184b6100e097bbb334c54078cd3c10d04b0b7c576eef27",
                "end": 40
            }
        },
        "preprocess": {
            "input": {
                "self": {
                    "value": "_empty",
                    "type": "_empty"
                },
                "text": {
                    "value": "_empty",
                    "type": "_empty"
                },
                "threshold": {
                    "value": 1000,
                    "type": "int"
                }
            },
            "output": {
                "value": null,
                "type": "None"
            },
            "docs": null,
            "cost": 1,
            "name": "preprocess",
            "source": {
                "start": 46,
                "length": 27,
                "path": "~/commune/commune/modules/agent/agent/agent.py",
                "code": null,
                "hash": "sha256:c1155bccf6e9cba857e3209932a561dfb8f68456d3e0c24c5be42d41f8c45fd3",
                "end": 73
            }
        },
        "resolve_path": {
            "input": {
                "self": {
                    "value": "_empty",
                    "type": "_empty"
                },
                "path": {
                    "value": "_empty",
                    "type": "_empty"
                }
            },
            "output": {
                "value": null,
                "type": "None"
            },
            "docs": null,
            "cost": 1,
            "name": "resolve_path",
            "source": {
                "start": 41,
                "length": 2,
                "path": "~/commune/commune/modules/agent/agent/agent.py",
                "code": null,
                "hash": "sha256:a7b6540dd85a4dfaef241592186f037c1c43c182f439f2e5d1b2dd712b562630",
                "end": 43
            }
        }
    },
    "name": "agent",
    "key": "5Fs5xerfwwjYTqjsjQ9aMyKF6HKq9qsVFPWZzDAvp1rdU5gs",
    "founder": "5GKvu9qC8VPjXnofUxZP6zxTmvzTBCY1vpJAkh6gWF8YxPKy",
    "cid": "sha256:0cc87f7a0cfa57021038c96d3729797d8e79e5a9dbc652fa745dee9a66dc5392",
    "time": 1746536167.8323998,
    "signature": "0xeacce5e297301c17a83c30227d4aa61ea2178aa1e209364777eabf89c7dc025f570905fe36fa9280ea282251ce59b71de7bb3471d04c50e0116101c531932382"
}